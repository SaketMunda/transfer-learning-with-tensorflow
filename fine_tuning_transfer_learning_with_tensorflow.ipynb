{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxDx8/gA9qou6C32P5IjhI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaketMunda/transfer-learning-with-tensorflow/blob/master/fine_tuning_transfer_learning_with_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning with TensorFlow : Fine Tuning\n",
        "\n",
        "In the previous section, we saw how we could leverage feature extraction transfer learning to get far better results on our Food vision project (using only 10% of original dataset) than building our own model from scratch.\n",
        "\n",
        "Now, we're going to cover another type of transfer learning: fine-tuning.\n",
        "\n",
        "In **fine-tuning transfer learning** the pre-trained weights from another model are unfrozen and tweaked during to better suit your own data.\n",
        "\n",
        "*Feature extraction transfer learning vs. fine-tuning transfer learning. The main difference between the two is that in fine-tuning, more layers of the pre-trained model get unfrozen and tuned on custom data. This fine-tuning usually takes more data than feature extraction to be effective.*"
      ],
      "metadata": {
        "id": "STYzc-8ZGhqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ðŸ’¡ **This time we will use the helper functions to speed up our steps in learning**"
      ],
      "metadata": {
        "id": "YHIrqFznIW1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the Helper functions"
      ],
      "metadata": {
        "id": "eE_H8bZLIq8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get helper_functions.py script from Github\n",
        "!wget https://raw.githubusercontent.com/SaketMunda/ml-helpers/master/helper_functions.py\n",
        "\n",
        "# Import helper functions we're going to use\n",
        "from helper_functions import create_tensorboard_callback, unzip_data, walk_through_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "id": "R9Nsxn6zI1lG",
        "outputId": "05a5d5c9-2349-44d7-c80b-b71d985f054c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-16 05:19:08--  https://raw.githubusercontent.com/SaketMunda/ml-helpers/master/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1937 (1.9K) [text/plain]\n",
            "Saving to: â€˜helper_functions.pyâ€™\n",
            "\n",
            "helper_functions.py 100%[===================>]   1.89K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-16 05:19:09 (35.7 MB/s) - â€˜helper_functions.pyâ€™ saved [1937/1937]\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-eeb9e7f6830d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Import helper functions we're going to use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhelper_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_tensorboard_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munzip_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwalk_through_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'create_tensorboard_callback' from 'helper_functions' (/content/helper_functions.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10 Food Classes : Working with Less data\n",
        "\n",
        "As in the previous notebook of Transfer Leanring : Feature Extraction, we got good results from less data (10%) of the training data using transfer learning from TensorFlow Hub.\n",
        "\n",
        "In this notebook, we're going to continue to work with smaller subsets of the data, except this time we'll have a look at how we can use in-built pretrained models within the `tf.keras.applications` module as well as how to fine-tune them to our own custom dataset.\n",
        "\n",
        "We'll also practice using a new but similar dataloader function to what we've used before, `image_dataset_from_directory()` which is a part of `tf.keras.preprocessing` module.\n",
        "\n",
        "Finally we'll be practicing using the [Keras Functional API](https://keras.io/guides/functional_api/) for building deep learning models. The functional API is a more flexible way to create models than the `tf.keras.Sequential`\n",
        "\n",
        "Let's start by downloading the data"
      ],
      "metadata": {
        "id": "9Z7N0tt_JC2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 10% of the data of the 10 classes\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
        "\n",
        "unzip_data('10_food_classes_10_percent.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl5ovR3QKRHo",
        "outputId": "83db7414-faf2-4a7b-b49d-5e38875ea6f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-16 04:20:42--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.189.128, 108.177.97.128, 108.177.125.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.189.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168546183 (161M) [application/zip]\n",
            "Saving to: â€˜10_food_classes_10_percent.zipâ€™\n",
            "\n",
            "10_food_classes_10_ 100%[===================>] 160.74M  30.8MB/s    in 6.1s    \n",
            "\n",
            "2022-12-16 04:20:48 (26.4 MB/s) - â€˜10_food_classes_10_percent.zipâ€™ saved [168546183/168546183]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Walk through the directory and list number of files\n",
        "walk_through_dir('10_food_classes_10_percent')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3SlAZCrKbAE",
        "outputId": "7b4fe8b6-6aa3-414d-9b03-a135559c64b0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2 directories and 0 files in '10_food_classes_10_percent'\n",
            "There are 10 directories and 0 files in '10_food_classes_10_percent/train'\n",
            "There are 0 directories and 75 files in '10_food_classes_10_percent/train/hamburger'\n",
            "There are 0 directories and 75 files in '10_food_classes_10_percent/train/chicken_curry'\n",
            "There are 0 directories and 75 files in '10_food_classes_10_percent/train/steak'\n",
            "There are 0 directories and 75 files in '10_food_classes_10_percent/train/grilled_salmon'\n",
            "There are 0 directories and 75 files in '10_food_classes_10_percent/train/chicken_wings'\n",
            "There are 0 directories and 75 files in '10_food_classes_10_percent/train/pizza'\n",
            "There are 0 directories and 75 files in '10_food_classes_10_percent/train/fried_rice'\n",
            "There are 0 directories and 75 files in '10_food_classes_10_percent/train/ice_cream'\n",
            "There are 0 directories and 75 files in '10_food_classes_10_percent/train/sushi'\n",
            "There are 0 directories and 75 files in '10_food_classes_10_percent/train/ramen'\n",
            "There are 10 directories and 0 files in '10_food_classes_10_percent/test'\n",
            "There are 0 directories and 250 files in '10_food_classes_10_percent/test/hamburger'\n",
            "There are 0 directories and 250 files in '10_food_classes_10_percent/test/chicken_curry'\n",
            "There are 0 directories and 250 files in '10_food_classes_10_percent/test/steak'\n",
            "There are 0 directories and 250 files in '10_food_classes_10_percent/test/grilled_salmon'\n",
            "There are 0 directories and 250 files in '10_food_classes_10_percent/test/chicken_wings'\n",
            "There are 0 directories and 250 files in '10_food_classes_10_percent/test/pizza'\n",
            "There are 0 directories and 250 files in '10_food_classes_10_percent/test/fried_rice'\n",
            "There are 0 directories and 250 files in '10_food_classes_10_percent/test/ice_cream'\n",
            "There are 0 directories and 250 files in '10_food_classes_10_percent/test/sushi'\n",
            "There are 0 directories and 250 files in '10_food_classes_10_percent/test/ramen'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's the same number of files and classes we used in previous notebook."
      ],
      "metadata": {
        "id": "Qz_b4_CSKotm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the training and testing directory\n",
        "train_dir = '10_food_classes_10_percent/train/'\n",
        "test_dir = '10_food_classes_10_percent/test/'"
      ],
      "metadata": {
        "id": "pKTTG2hDLrBi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've got some image data, we need a way of loading it into a Tensorflow compatible format.\n",
        "\n",
        "Previously, we've used the `ImageDataGenerator` class. And while this works well and is still very commonly used, this time we're going to use the `image_dataset_from_directory` function.\n",
        "\n",
        "One of the main benefits of using `tf.keras.preprocessing.image_dataset_from_directory()` rather than `ImageDataGenerator` is that it creates a `tf.data.Dataset` object rather than a generator. The main advantage of this is the `tf.data.Dataset` API is much faster and efficient than the `ImageDataGenerator` API which is paramount for larger datasets.\n",
        "\n"
      ],
      "metadata": {
        "id": "5W-OTNfVL9wE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "IMG_SHAPE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir,\n",
        "                                                  image_size=IMG_SHAPE,\n",
        "                                                  label_mode='categorical',\n",
        "                                                  batch_size=BATCH_SIZE)\n",
        "\n",
        "test_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(test_dir,\n",
        "                                                 image_size=IMG_SHAPE,\n",
        "                                                 label_mode='categorical',\n",
        "                                                 batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSN8trhhMway",
        "outputId": "c39563f9-4606-4828-ffb0-4113832c1ded"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 750 files belonging to 10 classes.\n",
            "Found 2500 files belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wonderful ! Looks like our dataloaders have found the correct number of images for each Dataset.\n",
        "\n",
        "For now, the main parameters we're concerned about in the `image_dataset_from_directory` function are:\n",
        "- `directory`\n",
        "- `image_size`\n",
        "- `batch_size`"
      ],
      "metadata": {
        "id": "fxpB4afwNRV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if we check the datatype of the preprocessed dataset\n",
        "train_data_10_percent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSPzMR7aOHaD",
        "outputId": "8d1005d3-5c1d-4898-9384-ee06692edb1b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's Batch Dataset"
      ],
      "metadata": {
        "id": "x3n5eA58ONgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check the classes\n",
        "train_data_10_percent.class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEPxjDtmOcEf",
        "outputId": "c330c96e-4d99-47de-c588-5b9e77741461"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chicken_curry',\n",
              " 'chicken_wings',\n",
              " 'fried_rice',\n",
              " 'grilled_salmon',\n",
              " 'hamburger',\n",
              " 'ice_cream',\n",
              " 'pizza',\n",
              " 'ramen',\n",
              " 'steak',\n",
              " 'sushi']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or if we wanted to see an example of batch of data, we could use the `take()` method"
      ],
      "metadata": {
        "id": "c05996fLOfDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# See an example batch of data\n",
        "for images, labels in train_data_10_percent.take(1):\n",
        "  print(images, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDp1gSWqOsq3",
        "outputId": "95334e5e-6531-43ae-f2f9-26cfc029376c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[[2.18262909e+02 2.20262909e+02 2.18548630e+02]\n",
            "   [2.29039383e+02 2.32110809e+02 2.36825104e+02]\n",
            "   [2.29207581e+02 2.33408005e+02 2.46341202e+02]\n",
            "   ...\n",
            "   [7.49052277e+01 6.73337555e+01 5.61194954e+01]\n",
            "   [7.19239349e+01 6.19239349e+01 5.19239349e+01]\n",
            "   [6.37116776e+01 5.37116776e+01 4.37116776e+01]]\n",
            "\n",
            "  [[2.28361130e+02 2.30361130e+02 2.28563614e+02]\n",
            "   [2.34136154e+02 2.37746170e+02 2.40720993e+02]\n",
            "   [2.33700409e+02 2.35557068e+02 2.48084351e+02]\n",
            "   ...\n",
            "   [7.30350189e+01 6.44635468e+01 5.52492867e+01]\n",
            "   [7.34839478e+01 6.34839516e+01 5.44839516e+01]\n",
            "   [6.31825066e+01 5.31825066e+01 4.41825066e+01]]\n",
            "\n",
            "  [[2.40898758e+02 2.42898758e+02 2.39898758e+02]\n",
            "   [2.32969070e+02 2.33210617e+02 2.35470337e+02]\n",
            "   [2.28244095e+02 2.27255264e+02 2.36729263e+02]\n",
            "   ...\n",
            "   [7.18604126e+01 6.32889442e+01 5.56237869e+01]\n",
            "   [7.14087067e+01 6.21832657e+01 5.47323723e+01]\n",
            "   [6.52837906e+01 5.60583420e+01 4.86074524e+01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[9.35490723e+01 1.10549072e+02 1.30098145e+02]\n",
            "   [9.40981445e+01 1.10679428e+02 1.30437851e+02]\n",
            "   [9.87440414e+01 1.10540932e+02 1.32507401e+02]\n",
            "   ...\n",
            "   [3.47569275e+01 4.21029358e+01 3.18388386e+01]\n",
            "   [4.14901543e+01 4.64901543e+01 4.01665459e+01]\n",
            "   [3.94467049e+01 4.62212410e+01 3.91230965e+01]]\n",
            "\n",
            "  [[7.20252609e+01 8.90252609e+01 1.05154716e+02]\n",
            "   [7.32966309e+01 8.92411499e+01 1.05430710e+02]\n",
            "   [6.78088913e+01 8.03803253e+01 9.81526337e+01]\n",
            "   ...\n",
            "   [4.14323845e+01 4.65341034e+01 4.19391289e+01]\n",
            "   [4.28617401e+01 4.69264679e+01 4.57322845e+01]\n",
            "   [4.22046013e+01 4.63340569e+01 4.51398735e+01]]\n",
            "\n",
            "  [[6.99923782e+01 8.73472672e+01 9.67668228e+01]\n",
            "   [7.49187012e+01 9.15638123e+01 1.02628479e+02]\n",
            "   [7.82732544e+01 9.04162140e+01 1.03413971e+02]\n",
            "   ...\n",
            "   [7.96393509e+01 8.36393509e+01 8.39295731e+01]\n",
            "   [8.10424042e+01 8.50424042e+01 8.73326263e+01]\n",
            "   [8.07613220e+01 8.47613220e+01 8.70515442e+01]]]\n",
            "\n",
            "\n",
            " [[[2.63571415e+01 1.83571415e+01 5.35714293e+00]\n",
            "   [2.90459175e+01 2.10459175e+01 8.04591846e+00]\n",
            "   [2.96428566e+01 2.16428566e+01 8.64285755e+00]\n",
            "   ...\n",
            "   [2.22571442e+02 2.26357147e+02 2.08928558e+02]\n",
            "   [2.23622452e+02 2.26622452e+02 2.07622452e+02]\n",
            "   [2.19127563e+02 2.22127563e+02 2.03127563e+02]]\n",
            "\n",
            "  [[2.73571434e+01 1.93571434e+01 6.35714293e+00]\n",
            "   [3.00051022e+01 2.20051022e+01 9.00510216e+00]\n",
            "   [3.00153065e+01 2.20153065e+01 9.01530647e+00]\n",
            "   ...\n",
            "   [2.23142868e+02 2.26928574e+02 2.09499985e+02]\n",
            "   [2.25785751e+02 2.28785751e+02 2.09785751e+02]\n",
            "   [2.23117355e+02 2.26117355e+02 2.07117355e+02]]\n",
            "\n",
            "  [[2.77857151e+01 1.93571434e+01 8.57142830e+00]\n",
            "   [2.94132652e+01 2.09846935e+01 1.01989794e+01]\n",
            "   [2.85969391e+01 2.01683674e+01 9.38265324e+00]\n",
            "   ...\n",
            "   [2.26831635e+02 2.30617340e+02 2.13188751e+02]\n",
            "   [2.26270401e+02 2.29270401e+02 2.10270401e+02]\n",
            "   [2.20352066e+02 2.23352066e+02 2.04352066e+02]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[3.58779564e+01 3.02298965e+01 1.48726864e+01]\n",
            "   [2.52757225e+01 2.03470631e+01 6.48989868e+00]\n",
            "   [1.41225462e+01 1.27653160e+01 1.68372124e-01]\n",
            "   ...\n",
            "   [1.09020416e+02 8.08061218e+01 4.07142944e+01]\n",
            "   [1.04612160e+02 7.56121597e+01 3.51836319e+01]\n",
            "   [1.06933640e+02 7.39234161e+01 3.50000000e+01]]\n",
            "\n",
            "  [[7.00715179e+01 5.35255547e+01 3.30969696e+01]\n",
            "   [5.93062706e+01 4.62296982e+01 2.68878136e+01]\n",
            "   [4.75716248e+01 3.64542580e+01 1.77858467e+01]\n",
            "   ...\n",
            "   [1.14647957e+02 8.63877335e+01 4.71887512e+01]\n",
            "   [1.10637779e+02 7.88367081e+01 3.97704010e+01]\n",
            "   [1.07071503e+02 7.30715027e+01 3.50715027e+01]]\n",
            "\n",
            "  [[7.96173782e+01 5.71581497e+01 3.38009720e+01]\n",
            "   [7.57857132e+01 5.52652397e+01 3.16887398e+01]\n",
            "   [7.25511322e+01 5.55612793e+01 3.29031296e+01]\n",
            "   ...\n",
            "   [1.10285583e+02 8.14284058e+01 4.26427002e+01]\n",
            "   [1.13617195e+02 8.16171951e+01 4.36171951e+01]\n",
            "   [1.09055939e+02 7.50559387e+01 3.80559387e+01]]]\n",
            "\n",
            "\n",
            " [[[4.74132652e+01 2.04132652e+01 1.14132652e+01]\n",
            "   [4.89285736e+01 2.19285717e+01 1.29285717e+01]\n",
            "   [5.00000000e+01 2.30000000e+01 1.40000000e+01]\n",
            "   ...\n",
            "   [9.94862139e-01 2.99486208e+00 2.75482178e-01]\n",
            "   [2.14287019e+00 3.78572750e+00 2.55148765e-02]\n",
            "   [2.99996519e+00 3.99996519e+00 0.00000000e+00]]\n",
            "\n",
            "  [[4.69744911e+01 1.99744892e+01 1.29744902e+01]\n",
            "   [4.88622475e+01 2.18622456e+01 1.48622456e+01]\n",
            "   [4.89846954e+01 2.19846935e+01 1.49846935e+01]\n",
            "   ...\n",
            "   [1.68881440e+00 2.87246919e+00 3.06091160e-02]\n",
            "   [3.85714293e+00 4.85714293e+00 0.00000000e+00]\n",
            "   [2.52547789e+00 3.52547789e+00 0.00000000e+00]]\n",
            "\n",
            "  [[4.61377563e+01 1.91377544e+01 1.21377554e+01]\n",
            "   [4.80000000e+01 2.10000000e+01 1.40000000e+01]\n",
            "   [4.80000000e+01 2.10000000e+01 1.40000000e+01]\n",
            "   ...\n",
            "   [3.99997807e+00 4.78571415e+00 5.96878052e-01]\n",
            "   [2.00000000e+00 3.00000000e+00 0.00000000e+00]\n",
            "   [4.42349625e+00 3.85206795e+00 2.80639589e-01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.60362167e+02 1.60576462e+02 1.61005051e+02]\n",
            "   [1.64510162e+02 1.66295868e+02 1.65938751e+02]\n",
            "   [1.59306152e+02 1.63642868e+02 1.62903091e+02]\n",
            "   ...\n",
            "   [1.31168381e+02 1.22168381e+02 1.15168381e+02]\n",
            "   [1.30928558e+02 1.21928558e+02 1.14928558e+02]\n",
            "   [1.27923462e+02 1.18923462e+02 1.11923462e+02]]\n",
            "\n",
            "  [[1.56357193e+02 1.56428604e+02 1.58428604e+02]\n",
            "   [1.60147934e+02 1.60219345e+02 1.62219345e+02]\n",
            "   [1.60882645e+02 1.59311111e+02 1.62168289e+02]\n",
            "   ...\n",
            "   [1.31056107e+02 1.22056107e+02 1.15056107e+02]\n",
            "   [1.30928558e+02 1.21928558e+02 1.14928558e+02]\n",
            "   [1.27974495e+02 1.18974495e+02 1.11974495e+02]]\n",
            "\n",
            "  [[1.56025497e+02 1.54025497e+02 1.55739792e+02]\n",
            "   [1.57010208e+02 1.55010208e+02 1.56724503e+02]\n",
            "   [1.64520401e+02 1.61448959e+02 1.63520401e+02]\n",
            "   ...\n",
            "   [1.31137741e+02 1.22137741e+02 1.15137741e+02]\n",
            "   [1.29928558e+02 1.20928558e+02 1.13928558e+02]\n",
            "   [1.28872437e+02 1.19872437e+02 1.12872437e+02]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[1.26428576e+01 4.64285707e+00 1.64285719e+00]\n",
            "   [1.29744902e+01 4.97448969e+00 1.97448993e+00]\n",
            "   [1.48571434e+01 6.85714293e+00 3.85714293e+00]\n",
            "   ...\n",
            "   [2.54403122e+02 2.47842194e+02 2.41413666e+02]\n",
            "   [2.51811249e+02 2.54668365e+02 2.45525482e+02]\n",
            "   [2.45683319e+02 2.47968964e+02 2.30611786e+02]]\n",
            "\n",
            "  [[1.40000000e+01 7.00000000e+00 1.00000000e+00]\n",
            "   [1.40000000e+01 7.00000000e+00 1.00000000e+00]\n",
            "   [1.42857141e+01 7.28571415e+00 1.28571415e+00]\n",
            "   ...\n",
            "   [2.55000000e+02 2.53637894e+02 2.47796143e+02]\n",
            "   [2.51147980e+02 2.55000000e+02 2.48586685e+02]\n",
            "   [2.42167984e+02 2.42453629e+02 2.26933090e+02]]\n",
            "\n",
            "  [[1.74285717e+01 8.42857170e+00 3.42857170e+00]\n",
            "   [1.55000000e+01 6.50000000e+00 1.50000000e+00]\n",
            "   [1.52142859e+01 6.21428585e+00 1.21428585e+00]\n",
            "   ...\n",
            "   [2.51596878e+02 2.54234772e+02 2.48500168e+02]\n",
            "   [2.49943909e+02 2.55000000e+02 2.49515259e+02]\n",
            "   [2.46663010e+02 2.44076218e+02 2.31234299e+02]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[1.67443893e+02 4.88724251e+01 7.21940184e+00]\n",
            "   [1.69280670e+02 5.32092361e+01 6.42352057e+00]\n",
            "   [1.74969254e+02 5.93978310e+01 7.75497341e+00]\n",
            "   ...\n",
            "   [2.50168350e+02 1.41168350e+02 5.81683540e+01]\n",
            "   [2.47872421e+02 1.38872421e+02 5.38724251e+01]\n",
            "   [2.44719345e+02 1.35719345e+02 5.07193527e+01]]\n",
            "\n",
            "  [[1.73382751e+02 5.90052605e+01 5.57659721e+00]\n",
            "   [2.03882706e+02 9.28878403e+01 3.64540672e+01]\n",
            "   [1.64061234e+02 5.55459785e+01 0.00000000e+00]\n",
            "   ...\n",
            "   [2.48071442e+02 1.39071442e+02 5.60714417e+01]\n",
            "   [2.48005096e+02 1.39005096e+02 5.40051041e+01]\n",
            "   [2.46811218e+02 1.37811218e+02 5.28112221e+01]]\n",
            "\n",
            "  [[2.00730042e+02 9.63984756e+01 2.46585007e+01]\n",
            "   [1.90377228e+02 8.82853928e+01 1.52597713e+01]\n",
            "   [2.13363831e+02 1.13869080e+02 4.12261124e+01]\n",
            "   ...\n",
            "   [2.49505081e+02 1.40505081e+02 5.75050888e+01]\n",
            "   [2.48617310e+02 1.39617310e+02 5.46173058e+01]\n",
            "   [2.47285645e+02 1.38285645e+02 5.32856445e+01]]]\n",
            "\n",
            "\n",
            " [[[2.02142849e+01 2.22142849e+01 1.72142849e+01]\n",
            "   [1.95459175e+01 2.15459175e+01 1.65459175e+01]\n",
            "   [2.12142868e+01 2.32142868e+01 1.86428566e+01]\n",
            "   ...\n",
            "   [2.19947968e+01 2.65662689e+01 1.95662689e+01]\n",
            "   [2.01428833e+01 2.51428833e+01 1.81428833e+01]\n",
            "   [1.85712891e+01 2.35712891e+01 1.65712891e+01]]\n",
            "\n",
            "  [[1.88571434e+01 2.08571434e+01 1.58571434e+01]\n",
            "   [2.01377544e+01 2.21377544e+01 1.71377544e+01]\n",
            "   [2.12142868e+01 2.32142868e+01 1.86428566e+01]\n",
            "   ...\n",
            "   [2.16427917e+01 2.62142639e+01 1.92142639e+01]\n",
            "   [2.11428699e+01 2.61428699e+01 1.91428699e+01]\n",
            "   [1.81681957e+01 2.31681957e+01 1.61681957e+01]]\n",
            "\n",
            "  [[1.77040806e+01 1.97040806e+01 1.47040825e+01]\n",
            "   [2.16734695e+01 2.36734695e+01 1.86734695e+01]\n",
            "   [2.10459194e+01 2.30459194e+01 1.84744892e+01]\n",
            "   ...\n",
            "   [2.14744415e+01 2.60459137e+01 1.90459137e+01]\n",
            "   [2.13418598e+01 2.63418598e+01 1.93418598e+01]\n",
            "   [1.75712891e+01 2.25712891e+01 1.55712891e+01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[2.79336853e+01 2.99336853e+01 2.50867519e+01]\n",
            "   [3.28265762e+01 3.53979836e+01 2.67551365e+01]\n",
            "   [5.12092705e+01 5.23521080e+01 4.13061981e+01]\n",
            "   ...\n",
            "   [7.43316498e+01 7.33316498e+01 5.33316498e+01]\n",
            "   [7.60357742e+01 7.40357742e+01 5.24643059e+01]\n",
            "   [7.23772736e+01 7.03772736e+01 4.69486809e+01]]\n",
            "\n",
            "  [[2.43570557e+01 2.63570557e+01 2.32652416e+01]\n",
            "   [3.19999142e+01 3.29999161e+01 2.69336205e+01]\n",
            "   [5.78571281e+01 5.74285545e+01 4.69999847e+01]\n",
            "   ...\n",
            "   [8.59899750e+01 8.49899750e+01 6.40613861e+01]\n",
            "   [9.07246475e+01 8.87246475e+01 6.38572693e+01]\n",
            "   [8.07807770e+01 7.87807770e+01 5.37807808e+01]]\n",
            "\n",
            "  [[2.64438496e+01 2.84438496e+01 2.54438496e+01]\n",
            "   [2.90918274e+01 3.00918274e+01 2.41632557e+01]\n",
            "   [4.90102158e+01 4.85816460e+01 3.81530724e+01]\n",
            "   ...\n",
            "   [8.65767365e+01 8.60818481e+01 6.35665131e+01]\n",
            "   [8.75409164e+01 8.56123581e+01 6.03980293e+01]\n",
            "   [8.40507584e+01 8.30507584e+01 5.50507584e+01]]]\n",
            "\n",
            "\n",
            " [[[4.20867348e+01 4.60867348e+01 5.50867348e+01]\n",
            "   [4.39999962e+01 4.79999962e+01 5.69999962e+01]\n",
            "   [3.80000000e+01 4.17857132e+01 5.03571434e+01]\n",
            "   ...\n",
            "   [6.59286118e+01 1.35714064e+01 5.35714293e+00]\n",
            "   [6.83826599e+01 1.63826580e+01 5.38265753e+00]\n",
            "   [6.94847031e+01 1.74847069e+01 6.48470640e+00]]\n",
            "\n",
            "  [[4.50459175e+01 4.90459175e+01 5.80459175e+01]\n",
            "   [4.50765305e+01 4.90765305e+01 5.80765305e+01]\n",
            "   [4.61275520e+01 4.91275520e+01 5.61275520e+01]\n",
            "   ...\n",
            "   [6.98418427e+01 1.48724480e+01 7.85714340e+00]\n",
            "   [7.18571548e+01 1.68571568e+01 9.85715675e+00]\n",
            "   [7.37142868e+01 1.97142868e+01 9.71428680e+00]]\n",
            "\n",
            "  [[4.31428566e+01 4.61428566e+01 5.47142868e+01]\n",
            "   [4.41275520e+01 4.71275520e+01 5.41275520e+01]\n",
            "   [4.88112259e+01 5.11683693e+01 5.83826523e+01]\n",
            "   ...\n",
            "   [7.10000229e+01 1.34031000e+01 7.21426392e+00]\n",
            "   [7.38571548e+01 1.64285851e+01 9.64286995e+00]\n",
            "   [7.59286041e+01 1.85000343e+01 1.17143202e+01]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[5.53622360e+01 1.15713634e+01 7.92857170e+00]\n",
            "   [5.52551041e+01 1.40867329e+01 1.01428566e+01]\n",
            "   [5.45714493e+01 1.48316278e+01 1.22806635e+01]\n",
            "   ...\n",
            "   [1.02709808e+02 3.29953918e+01 2.89953918e+01]\n",
            "   [1.03944290e+02 3.57604599e+01 2.47604599e+01]\n",
            "   [1.03061569e+02 3.69185638e+01 2.05614166e+01]]\n",
            "\n",
            "  [[5.36173401e+01 1.17143116e+01 8.00001335e+00]\n",
            "   [5.40765190e+01 1.32091951e+01 9.14285660e+00]\n",
            "   [5.06428299e+01 1.07857141e+01 8.71427250e+00]\n",
            "   ...\n",
            "   [8.40457993e+01 1.33315668e+01 7.58659649e+00]\n",
            "   [8.55764542e+01 1.28622208e+01 5.72954512e+00]\n",
            "   [9.06428833e+01 1.79286499e+01 9.07153320e+00]]\n",
            "\n",
            "  [[5.25153275e+01 1.50561695e+01 9.28574944e+00]\n",
            "   [5.15459518e+01 1.44541187e+01 8.50003433e+00]\n",
            "   [4.77143555e+01 1.27908936e+01 9.21946526e+00]\n",
            "   ...\n",
            "   [8.15562363e+01 2.45564194e+01 1.32604456e+01]\n",
            "   [7.86635056e+01 2.10922146e+01 1.34493923e+01]\n",
            "   [7.78574219e+01 2.02861328e+01 1.46433105e+01]]]], shape=(32, 224, 224, 3), dtype=float32) tf.Tensor(\n",
            "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]], shape=(32, 10), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image arrays come out as tensors of pixel values where as the labels come out as one-hot encodings."
      ],
      "metadata": {
        "id": "cWmO7vv1PKka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 0. Building a transfer learning model using the Keras Functional API\n",
        "\n",
        "Alight, our data is tensor-ified, let's build a model.\n",
        "\n",
        "To do so, we're going to be using the `tf.keras.applications` module as it contains a series of already trained (on ImageNet) computer vision models as well as the Keras Functional API to construct our model.\n",
        "\n",
        "We're going to go through the following steps:\n",
        "\n",
        "1. Instantiate the pretrained model object by choosing a target model such as `EfficientNetB0` from `tf.keras.applications`, setting the `include_top` to `False` (we do this because we're going to create our own top, which are the output layers for the model).\n",
        "2. Set the base model's `trainable` attribute to `False` to freeze all of the weights in the pre-trained model.\n",
        "3. Define an input layer for our model, for example, what shape of data should our model expect ?\n",
        "4. [Optional] Normalize the inputs to our model if it requires. Some of comp vis models such as `ResNetV250` require their inputs to be between 0 & 1."
      ],
      "metadata": {
        "id": "pKbfEgiLPebN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a base model with tf.keras.applications\n",
        "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
        "\n",
        "# 2. Freeze the base model (so the pre-trained weights remain same)\n",
        "base_model.trainable = False\n",
        "\n",
        "# 3. Create the inputs into the base model\n",
        "inputs = tf.keras.layers.Input(shape=(224, 224, 3), name='input_layer')\n",
        "\n",
        "# 4. If using ResNet50V2, add this to speed up convergence, remove for EfficientNet\n",
        "# x = tf.keras.layers.experimental.preprocessing.Rescaling(1/255.)(inputs)\n",
        "\n",
        "# 5. Pass the inputs to our base model\n",
        "x = base_model(inputs)\n",
        "# check data shape after passing into base model\n",
        "print(f'Shape after base_model: {x.shape}')\n",
        "\n",
        "# 6. Average pool the outputs of the base model (aggregate all the most important information, reduce number of computation)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D(name='global_average_pooling_layer')(x)\n",
        "print(f'After Global Average Pooling 2D:{x.shape}')\n",
        "\n",
        "# 7. Create the output activation layer\n",
        "outputs = tf.keras.layers.Dense(10, activation='softmax', name='output_layer')(x)\n",
        "\n",
        "# 8. Combine the inputs and outputs into our model\n",
        "model_0 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# 9. Compile the model\n",
        "model_0.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                optimizer='Adam',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# 10. Fit the model\n",
        "history_0 = model_0.fit(train_data_10_percent,\n",
        "                        epochs=5,\n",
        "                        steps_per_epochs=len(train_data_10_percent),\n",
        "                        validation_data=test_data_10_percent,\n",
        "                        validation_steps=len(test_data_10_percent),\n",
        "                        callbacks=[create_tensorboard_callback('transfer_learning',\n",
        "                                                               '10_percent_feature_extraction')])"
      ],
      "metadata": {
        "id": "DE9axB3KSmMj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}